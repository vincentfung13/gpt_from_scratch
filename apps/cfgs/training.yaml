defaults:
  - _self_
  - override hydra/job_logging: custom

task_name: npt_training
run_name: pilot_run
device: cuda
save_dir: checkpoints/${run_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}

hydra:
  run:
    dir: ${save_dir}

data:
  train_file: data/tiny_stories_train.tokens.uint16.npy 
  val_file: data/tiny_stories_val.tokens.uint16.npy
  batch_size: 128 
  seq_len: 256 
  # This is use to save the tokenizer used to tokenize the training set
  # it's for tracking purposes and does not take effect in actual training
  tokenizer_path: data/tinystories_bpe_tokenizer
   
model:
  d_model: 512 
  d_ff: 1344 
  num_heads: 16
  vocab_size: 10000
  num_transformer_layers: 4
  rope_theta: 10000.0
  context_len: ${data.seq_len}

optim:
  lr: 3e-4 
  weight_decay: 0.1
  betas: [0.9, 0.999]
  eps: 1e-8
  # cosine warmup
  min_lr: 1e-5
  warmup_iters: 2000 
  cosine_cyle_iters: 10000 

trainer:
  total_steps: 10000
  val_freq: 1000
  val_steps: 500 
  save_freq: 1000
  log_freq: 10
  resume_checkpoint_path: null
  resume: false