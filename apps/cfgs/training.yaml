defaults:
  - _self_
  - override hydra/job_logging: custom

task_name: npt_training
run_name: pilot_run
device: cuda
save_dir: checkpoints/${run_name}/${now:%Y-%m-%d-%H-%M-%S}

hydra:
  run:
    dir: ${save_dir}

data:
  train_file: data/tiny_stories_train.tokens.fixed.uint16.npy 
  val_file: data/tiny_stories_val.tokens.fixed.uint16.npy
  batch_size: 128 
  seq_len: 256 
  # This is use to save the tokenizer used to tokenize the training set
  # it's for tracking purposes and does not take effect in actual training
  tokenizer_path: data/tinystories_bpe_tokenizer
   
model:
  d_model: 512 
  d_ff: 1344 
  num_heads: 16
  vocab_size: 10000
  num_transformer_layers: 4
  rope_theta: 10000.0
  context_len: ${data.seq_len}

trainer:
  total_steps: 10000 
  save_freq: 10000
  log_freq: 10
  resume_checkpoint_path: null
  resume: false

# AdawmW params
optim:
  lr: 5e-4
  weight_decay: 0.1
  betas: [0.9, 0.95]
  eps: 1e-5
  # grad accumulatioan
  grad_accumulation_steps: 1
  grad_clip_norm: 1.0
  # cosine warmup
  min_lr: 1e-5
  warmup_iters: 500 
  cosine_cycle_iters: 10000 

wandb:
  enable: true
  project: llm_from_scratch 
